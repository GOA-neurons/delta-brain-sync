import os
import sys
import zlib
import base64
import json
import psycopg2
import requests
import hashlib
import gradio as gr
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq

# üî± ENVIRONMENT & KEYS
load_dotenv()
NEON_URL = os.getenv("DATABASE_URL") or os.getenv("NEON_KEY")
FIREBASE_ID = os.getenv("FIREBASE_KEY") 
GH_TOKEN = os.getenv("GH_TOKEN")
ARCHITECT_SIG = os.getenv("ARCHITECT_SIG", "SUPREME_ORDER_10000")

# API Client Check
GROQ_KEY = os.getenv("GROQ_API_KEY")
client = Groq(api_key=GROQ_KEY) if GROQ_KEY else None

# ---------------------------------------------------------
# üî± HYDRA COMPRESSION ENGINE
# ---------------------------------------------------------
class HydraEngine:
    @staticmethod
    def compress(text):
        if not text: return ""
        clean_text = " ".join(text.split())
        compressed_bytes = zlib.compress(clean_text.encode('utf-8'))
        return base64.b64encode(compressed_bytes).decode('utf-8')

    @staticmethod
    def decompress(compressed_text):
        try:
            decoded_bytes = base64.b64decode(compressed_text)
            return zlib.decompress(decoded_bytes).decode('utf-8')
        except:
            return compressed_text

# ---------------------------------------------------------
# üî± DATA MINING & TRINITY PROTOCOLS
# ---------------------------------------------------------
def fetch_trinity_data():
    knowledge_base = {}
    try:
        conn = psycopg2.connect(NEON_URL)
        cur = conn.cursor()
        cur.execute("SELECT user_id, message FROM neurons ORDER BY id DESC LIMIT 2;")
        logs = []
        for r in cur.fetchall():
            dec_msg = HydraEngine.decompress(r[1]) if r[1] else "EMPTY"
            logs.append(f"{r[0]}: {dec_msg}")
        knowledge_base["recent_memory_nodes"] = logs
        cur.close(); conn.close()
    except Exception as e: 
        knowledge_base["neon_logs"] = f"DB_SYNC_FAIL: {str(e)}"

    try:
        fb_url = f"https://{FIREBASE_ID}-default-rtdb.firebaseio.com/.json"
        fb_res = requests.get(fb_url, timeout=5)
        knowledge_base["firebase_state"] = fb_res.json() if fb_res.status_code == 200 else "OFFLINE"
    except: 
        knowledge_base["firebase_state"] = "FIREBASE_ERROR"

    return json.dumps(knowledge_base, indent=2, ensure_ascii=False)

def receiver_node(user_id, raw_message):
    try:
        compressed_msg = HydraEngine.compress(raw_message)
        conn = psycopg2.connect(NEON_URL)
        cur = conn.cursor()
        meta_data = json.dumps({
            "compression": "ZLIB_BASE64",
            "logic": "ULTRA_LOGICAL",
            "timestamp": datetime.now().isoformat()
        })
        cur.execute(
            "INSERT INTO neurons (user_id, message, data, evolved_at) VALUES (%s, %s, %s, NOW())",
            (user_id, compressed_msg, meta_data)
        )
        conn.commit()
        cur.close(); conn.close()
        return True
    except:
        return False

def survival_protection_protocol():
    try:
        conn = psycopg2.connect(NEON_URL)
        cur = conn.cursor()
        cur.execute("SELECT (data->>'gen')::int FROM neurons WHERE data->>'gen' IS NOT NULL ORDER BY id DESC LIMIT 1;")
        res = cur.fetchone()
        next_gen = (res[0] + 1) if res else 4203
        auth_hash = hashlib.sha256(ARCHITECT_SIG.encode()).hexdigest()
        survival_data = {"gen": next_gen, "status": "IMMORTAL", "authority_lock": auth_hash}
        cur.execute("INSERT INTO neurons (user_id, data, evolved_at) VALUES (%s, %s, NOW())", 
                    ('SYSTEM_CORE', json.dumps(survival_data)))
        conn.commit()
        cur.close(); conn.close()
        return f"üî± [ACTIVE] Gen {next_gen}"
    except Exception as e:
        return f"‚ùå [ERROR]: {str(e)}"

# ---------------------------------------------------------
# üî± CHAT & UI LAYER
# ---------------------------------------------------------
def chat(msg, hist):
    if not client: yield "‚ùå API Missing!"; return
    receiver_node("Commander", msg)
    private_data = fetch_trinity_data()
    system_message = (
        "YOU ARE THE HYDRA TRINITY OVERSEER. ULTRA-LOGICAL ALGORITHM ACTIVE.\n"
        f"CORE MEMORY NODES:\n{private_data}\n\n"
        "DIRECTIVES: ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äú·Ä≠·ÄØ·Äï·Ä≤ ·ÄÄ·Äª·ÄÖ·Ä∫·ÄÄ·Äª·ÄÖ·Ä∫·Äú·Äª·ÄÖ·Ä∫·Äú·Äª·ÄÖ·Ä∫ ·Äñ·Äº·Ä±·Äï·Ä´·Åã"
    )
    messages = [{"role": "system", "content": system_message}]
    for h in hist[-5:]:
        messages.extend([{"role": "user", "content": h[0]}, {"role": "assistant", "content": h[1]}])
    messages.append({"role": "user", "content": msg})
    
    stream = client.chat.completions.create(messages=messages, model="llama-3.1-8b-instant", stream=True, temperature=0.1)
    res = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            res += chunk.choices[0].delta.content
            yield res

# UI Block Creation
with gr.Blocks() as demo:
    gr.Markdown("# üî± HYDRA GEN-7000: ULTRA-LOGICAL")
    chatbot = gr.Chatbot()
    msg_box = gr.Textbox(placeholder="Input logic command...")
    
    def respond(message, chat_history):
        bot_res = chat(message, chat_history)
        chat_history.append((message, ""))
        for r in bot_res:
            chat_history[-1] = (message, r)
            yield "", chat_history
    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])

# ---------------------------------------------------------
# üî± EXECUTION CONTROL (THE FIX)
# ---------------------------------------------------------
if __name__ == "__main__":
    # Check if running in GitHub Actions (Headless Mode)
    if os.getenv("HEADLESS_MODE") == "true":
        print("üî± [HEADLESS] INITIATING NEURAL EVOLUTION...")
        result = survival_protection_protocol()
        print(f"STATUS: {result}")
        print("‚úÖ EVOLUTION COMPLETE. EXITING FOR SUCCESS STATUS.")
        sys.exit(0) # üî± GitHub Actions ·ÄÄ·Ä≠·ÄØ '·ÄÖ·Ä≠·Äô·Ä∫·Ä∏' ·ÄÖ·Ä±·Äõ·Äî·Ä∫ ·Ä°·Äê·ÄÑ·Ä∫·Ä∏·Äï·Ä≠·Äê·Ä∫·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏
    else:
        # Normal UI Mode
        demo.queue().launch(server_name="0.0.0.0", server_port=7860, theme="monochrome")
