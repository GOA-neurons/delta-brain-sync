import os
import zlib
import base64
import psycopg2
import pandas as pd
import gradio as gr
from dotenv import load_dotenv
from groq import Groq
from datasets import load_dataset
from sqlalchemy import create_engine
from huggingface_hub import HfApi

# ğŸ”± áá‹ CORE INITIALIZATION
load_dotenv()
NEON_URL = os.getenv("NEON_KEY") or os.getenv("DATABASE_URL")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
HF_TOKEN = os.getenv("HF_TOKEN")

client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
engine = create_engine(NEON_URL) if NEON_URL else None

class HydraEngine:
    """á€™á€°á€œá€¡á€á€­á€¯á€„á€ºá€¸ Neural Data á€€á€­á€¯ Encode/Decode á€œá€¯á€•á€ºá€á€Šá€·á€º á€¡á€„á€ºá€‚á€»á€„á€º"""
    @staticmethod
    def compress(data):
        if not data: return ""
        return base64.b64encode(zlib.compress(data.encode('utf-8'))).decode('utf-8')
    @staticmethod
    def decompress(c):
        try: return zlib.decompress(base64.b64decode(c)).decode('utf-8')
        except: return str(c)

# ğŸ”± á‚á‹ DATA PIPELINE (SCIENCE & TECH DATA PUMP)
def universal_hyper_ingest(limit=50):
    """Hugging Face á€™á€¾ á€á€­á€•á€¹á€•á€¶á€’á€±á€á€¬á€™á€»á€¬á€¸á€€á€­á€¯ Neon á€‘á€²á€á€­á€¯á€· Neural Compression á€–á€¼á€„á€·á€º á€á€½á€„á€ºá€¸á€á€¼á€„á€ºá€¸"""
    if not engine: return "âŒ Database Offline"
    try:
        print("ğŸ“¡ Accessing ArXiv Repository (Parquet Format)...")
        # Parquet Mode á€–á€¼á€„á€·á€º á€’á€±á€á€¬á€¡á€™á€¾á€”á€ºá€á€€á€šá€º á€†á€½á€²á€šá€°á€á€¼á€„á€ºá€¸
        ds = load_dataset("arxiv_dataset", "full", split='train', streaming=True)
        records = []
        for i, entry in enumerate(ds):
            if i >= limit: break
            records.append({
                'science_domain': 'Global_Expansion',
                'title': entry.get('title'),
                'detail': HydraEngine.compress(entry.get('abstract', '')),
                'energy_stability': -500.0,
                'master_sequence': entry.get('categories')
            })
            print(f"ğŸ“¥ Buffer: {entry.get('title')[:40]}...")

        if records:
            # Atomic Write to Database
            pd.DataFrame(records).to_sql('genesis_pipeline', engine, if_exists='append', index=False)
            return f"âœ… SUCCESS: {len(records)} Neural Records Synced to Neon."
        return "âš ï¸ Sync Failed: No Data Fetched."
    except Exception as e:
        return f"âŒ Pipeline Crash: {str(e)}"

# ğŸ”± áƒá‹ DIRECT API SYNC (NO GIT ERROR)
def sync_to_huggingface():
    """Git Push á€™á€œá€­á€¯á€˜á€² API á€–á€¼á€„á€·á€º á€–á€­á€¯á€„á€ºá€™á€»á€¬á€¸á€€á€­á€¯ Hugging Face Space á€á€­á€¯á€· á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€á€„á€ºá€á€¼á€„á€ºá€¸"""
    if not HF_TOKEN: return
    try:
        api = HfApi()
        api.upload_folder(
            folder_path=".",
            repo_id="TELEFOXX/GOA",
            repo_type="space",
            token=HF_TOKEN,
            ignore_patterns=[".git*", "__pycache__*"]
        )
        print("ğŸ”± Space Sync Complete: Code & UI Updated.")
    except Exception as e:
        print(f"âŒ Sync Failed: {e}")

# ğŸ”± á„á‹ OMNI-OVERSEER CHAT LOGIC
def fetch_neon_context():
    """Database á€™á€¾ á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€› á€á€­á€•á€¹á€•á€¶á€’á€±á€á€¬á€™á€»á€¬á€¸á€€á€­á€¯ Context á€¡á€–á€¼á€…á€º á€šá€°á€á€¼á€„á€ºá€¸"""
    try:
        conn = psycopg2.connect(NEON_URL, connect_timeout=5)
        cur = conn.cursor()
        cur.execute("""
            (SELECT user_id, message FROM neurons ORDER BY id DESC LIMIT 3)
            UNION ALL
            (SELECT science_domain, detail FROM genesis_pipeline ORDER BY id DESC LIMIT 3)
        """)
        rows = cur.fetchall()
        cur.close(); conn.close()
        return " | ".join([f"[{r[0]}]: {HydraEngine.decompress(r[1])}" for r in rows])
    except: return "Standby Mode"

def stream_logic(msg, hist):
    context = fetch_neon_context()
    messages = [{"role": "system", "content": f"CONTEXT: {context}\ná€™á€„á€ºá€¸á€€ TelefoxX Overseer á€–á€¼á€…á€ºá€á€šá€ºá‹ á€™á€¼á€”á€ºá€™á€¬á€œá€­á€¯ á€–á€¼á€±á€†á€­á€¯á€•á€«á‹"}]
    for h in hist: 
        if h[0]: messages.append({"role": "user", "content": h[0]})
        if h[1]: messages.append({"role": "assistant", "content": h[1]})
    messages.append({"role": "user", "content": msg})
    
    completion = client.chat.completions.create(model="llama-3.1-8b-instant", messages=messages, stream=True)
    ans = ""
    for chunk in completion:
        if chunk.choices[0].delta.content:
            ans += chunk.choices[0].delta.content
            yield ans

# ğŸ”± á…á‹ UI SETUP (GRADIO MONOCHROME)
with gr.Blocks(theme="monochrome", title="TELEFOXX OMNI-SYNC") as demo:
    gr.Markdown("# ğŸ”± TELEFOXX OMNI-SYNC CORE\n**Status:** Operational")
    with gr.Tab("Omni-Overseer"):
        chatbot = gr.Chatbot()
        msg_input = gr.Textbox(placeholder="á€¡á€™á€­á€”á€·á€ºá€•á€±á€¸á€•á€« Commander...")
        def user(m, h): return "", h + [[m, None]]
        def bot(h):
            for r in stream_logic(h[-1][0], h[:-1]):
                h[-1][1] = r
                yield h
        msg_input.submit(user, [msg_input, chatbot], [msg_input, chatbot], queue=False).then(bot, chatbot, chatbot)

    with gr.Tab("Expansion Control"):
        status_box = gr.Textbox(label="Expansion Status")
        gr.Button("ğŸš€ Trigger Global Expansion").click(universal_hyper_ingest, [], status_box)

# ğŸ”± á†á‹ EXECUTION CONTROL
if __name__ == "__main__":
    if os.getenv("HEADLESS_MODE") == "true":
        print("ğŸ”± TRIGGERING DATA PUMP & SYNC...")
        print(universal_hyper_ingest(limit=50))
        sync_to_huggingface()
        os._exit(0)
    else:
        demo.launch(server_name="0.0.0.0", server_port=7860)
