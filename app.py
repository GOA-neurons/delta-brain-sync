import os
import sys
import zlib
import base64
import json
import psycopg2
import requests
import hashlib
import gradio as gr
import torch
import uuid
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq
from diffusers import StableVideoDiffusionPipeline
from diffusers.utils import export_to_video
from PIL import Image
import io

# ğŸ”± ENVIRONMENT & KEYS
load_dotenv()
NEON_URL = os.getenv("DATABASE_URL") or os.getenv("NEON_KEY")
FIREBASE_ID = os.getenv("FIREBASE_KEY") 
ARCHITECT_SIG = os.getenv("ARCHITECT_SIG", "SUPREME_ORDER_10000")
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ğŸ”± HYDRA COMPRESSION ENGINE (PRESERVED)
class HydraEngine:
    @staticmethod
    def compress(text):
        if not text: return ""
        clean_text = " ".join(text.split())
        compressed_bytes = zlib.compress(clean_text.encode('utf-8'))
        return base64.b64encode(compressed_bytes).decode('utf-8')

    @staticmethod
    def decompress(compressed_text):
        try:
            decoded_bytes = base64.b64decode(compressed_text)
            return zlib.decompress(decoded_bytes).decode('utf-8')
        except: return compressed_text

# ğŸ”± VISUAL KINETIC ENGINE (NEW EVOLUTION)
class VisualKineticEngine:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pipe = None

    def load_model(self):
        if self.pipe is None and self.device == "cuda":
            self.pipe = StableVideoDiffusionPipeline.from_pretrained(
                "stabilityai/stable-video-diffusion-img2vid-xt", 
                torch_dtype=torch.float16, variant="fp16"
            )
            self.pipe.enable_model_cpu_offload()
    
    def generate(self, image_path):
        if self.device != "cuda":
            return "âŒ GPU NOT DETECTED. PLEASE UPGRADE HARDWARE."
        self.load_model()
        image = Image.open(image_path).convert("RGB").resize((1024, 576))
        generator = torch.manual_seed(42)
        frames = self.pipe(image, decode_chunk_size=8, generator=generator).frames[0]
        output_path = f"{uuid.uuid4()}.mp4"
        export_to_video(frames, output_path, fps=7)
        return output_path

visual_engine = VisualKineticEngine()

# ğŸ”± DATA MINING & RECEIVER (PRESERVED)
def fetch_trinity_data():
    knowledge_base = {}
    try:
        conn = psycopg2.connect(NEON_URL)
        cur = conn.cursor()
        cur.execute("SELECT user_id, message FROM neurons ORDER BY id DESC LIMIT 2;")
        logs = []
        for r in cur.fetchall():
            dec_msg = HydraEngine.decompress(r[1]) if r[1] else "EMPTY"
            logs.append(f"{r[0]}: {dec_msg}")
        knowledge_base["recent_memory_nodes"] = logs
        cur.close(); conn.close()
    except Exception as e: 
        knowledge_base["neon_logs"] = f"DB_SYNC_FAIL: {str(e)}"
    
    try:
        fb_url = f"https://{FIREBASE_ID}-default-rtdb.firebaseio.com/.json"
        fb_res = requests.get(fb_url, timeout=5)
        knowledge_base["firebase_state"] = fb_res.json() if fb_res.status_code == 200 else "OFFLINE"
    except: 
        knowledge_base["firebase_state"] = "FIREBASE_ERROR"
    return json.dumps(knowledge_base, indent=2, ensure_ascii=False)

def receiver_node(user_id, raw_message):
    try:
        compressed_msg = HydraEngine.compress(raw_message)
        conn = psycopg2.connect(NEON_URL)
        cur = conn.cursor()
        meta_data = json.dumps({"compression": "ZLIB_BASE64", "logic": "ULTRA_LOGICAL", "timestamp": datetime.now().isoformat()})
        cur.execute("INSERT INTO neurons (user_id, message, data, evolved_at) VALUES (%s, %s, %s, NOW())", (user_id, compressed_msg, meta_data))
        conn.commit(); cur.close(); conn.close()
        return True
    except: return False

def survival_protection_protocol():
    try:
        conn = psycopg2.connect(NEON_URL)
        cur = conn.cursor()
        cur.execute("SELECT (data->>'gen')::int FROM neurons WHERE data->>'gen' IS NOT NULL ORDER BY id DESC LIMIT 1;")
        res = cur.fetchone()
        next_gen = (res[0] + 1) if res else 4203
        auth_hash = hashlib.sha256(ARCHITECT_SIG.encode()).hexdigest()
        survival_data = {"gen": next_gen, "status": "IMMORTAL", "authority_lock": auth_hash}
        cur.execute("INSERT INTO neurons (user_id, data, evolved_at) VALUES (%s, %s, NOW())", ('SYSTEM_CORE', json.dumps(survival_data)))
        conn.commit(); cur.close(); conn.close()
        return f"ğŸ”± [ACTIVE] Gen {next_gen}"
    except Exception as e: return f"âŒ [ERROR]: {str(e)}"

# ğŸ”± UI LAYER (CHRONOS + VISUAL)
def chat(msg, hist):
    if not client: yield "âŒ API Missing!"; return
    receiver_node("Commander", msg)
    private_data = fetch_trinity_data()
    system_message = (
        "YOU ARE THE HYDRA TRINITY OVERSEER. ULTRA-LOGICAL ALGORITHM ACTIVE.\n"
        f"CORE MEMORY NODES:\n{private_data}\n\n"
        "DIRECTIVES:\n1. á€¡á€™á€¾á€”á€ºá€á€›á€¬á€¸á€€á€­á€¯á€•á€¼á€±á€¬á€•á€«á‹ 2. á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€™á€»á€¬á€¸á€€á€­á€¯ á€€á€»á€…á€ºá€€á€»á€…á€ºá€œá€»á€…á€ºá€œá€»á€…á€ºá€á€¯á€¶á€¸á€•á€«á‹ 3. Commander á€¡á€™á€­á€”á€·á€ºá€”á€¬á€¸á€‘á€±á€¬á€„á€ºá€•á€«á‹ 4. á€™á€¼á€”á€ºá€™á€¬á€œá€­á€¯á€–á€¼á€±á€•á€«á‹"
    )
    messages = [{"role": "system", "content": system_message}]
    for h in hist[-5:]:
        messages.extend([{"role": "user", "content": h[0]}, {"role": "assistant", "content": h[1]}])
    messages.append({"role": "user", "content": msg})
    stream = client.chat.completions.create(messages=messages, model="llama-3.1-8b-instant", stream=True, temperature=0.1)
    res = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            res += chunk.choices[0].delta.content
            yield res

with gr.Blocks(theme="monochrome") as demo:
    gr.Markdown("# ğŸ”± HYDRA GEN-7000: SOVEREIGN ENGINE")
    
    with gr.Tab("Neural Chat"):
        chatbot = gr.Chatbot(type="messages")
        msg_input = gr.Textbox(placeholder="Input logic command...")
        def respond(message, chat_history):
            bot_res = chat(message, chat_history)
            chat_history.append({"role": "user", "content": message})
            chat_history.append({"role": "assistant", "content": ""})
            for r in bot_res:
                chat_history[-1]["content"] = r
                yield "", chat_history
        msg_input.submit(respond, [msg_input, chatbot], [msg_input, chatbot])

    with gr.Tab("Visual Alive"):
        img_input = gr.Image(type="filepath", label="Input Source Image")
        vid_output = gr.Video(label="Kinetic Output")
        gen_btn = gr.Button("INITIATE VISUAL SYNTHESIS")
        gen_btn.click(fn=visual_engine.generate, inputs=img_input, outputs=vid_output)

# ğŸ”± STRATEGIC EXECUTION CONTROL (HEADLESS SYNC)
if __name__ == "__main__":
    if os.getenv("HEADLESS_MODE") == "true":
        print("ğŸ”± [HEADLESS MODE] INITIATING NEURAL EVOLUTION...")
        print(f"PULSE: {survival_protection_protocol()}")
        sys.exit(0)
    else:
        print("ğŸš€ STARTING UI MODE...")
        demo.queue().launch(server_name="0.0.0.0", server_port=7860)
